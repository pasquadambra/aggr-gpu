\section{Sparse Linear Algebra and Preconditioners on GPUs}

General Purpose Graphics Processing Units (GPGPUs)~\cite{Luebke06}  
are today an established and attractive choice in the world of
scientific computing, found in many among the fastest supercomputers
on the Top~500 list, and offered in standard Cloud infrastructure
services, e.g. in  Amazon EC2. It is therefore not surprising that
they are at the focus of many research efforts revolving around the
efficient implementation of scientific software, including solvers for
sparse linear system of equations. 


From a  software point of view, iterative methods employ the matrix $A$ 
to perform matrix-vector products  $y\gets Ax$.
The SpMV kernel is well-known to be a memory-bounded application;
and  its bandwidth usage  is strongly dependent on both the input
matrix and on the underlying computing platform(s). For a detailed
overview of the implementation issues and a survey of available
techniques for this kernel on GPUs,
see~\cite{Filippone:2017:SMM:3034774.3017994}.    

Additional considerations are necessary when implementing
preconditioning operations on GPUs. First of all, implementing the
solution of sparse triangular systems on GPUs is extremely difficult.
As an example, the conjugate gradient preconditioned with incomplete
LU available in CUDA CuSPARSE since version 4.0 
is reported in~\cite{Naumov11} as achieving at best a speedup factor
of about 2 over a standard CPU implementation. The main reason is that
the GPU requires a massive amount of data parallelism to be exploited,
and in the sparse triangular factors the amount of available
parallelism is limited by the sparsity. It is therefore appealing to
use preconditioners employing matrix-vector products as their main
kernel, among them  Point-Jacobi iterations and preconditioning
through sparse approximation of the   inverses.
For a discussion of sparse approximate inverses on GPUs 
see~\cite{BERTACCINI2016693}. 

The implementation of the multilevel preconditioners for which we will
present results in this paper is based on MLD2P4~\cite{mld-toms} and
PSBLAS~\cite{psblas3}. Thanks to the modular architecture of both
packages, it is possible to define \emph{plugins} for 
\begin{itemize}
\item Operators on the GPU, including sparse matrix-vector product
  supporting multiple storage formats and vector-vector operations;
\item Approximate inverses as local solvers.
\end{itemize}
By combining these ingredients in a multilevel framework we can obtain
interesting scalability results. 

Three additional practical obstacles must be mentioned here. 
First of all,  because the allocation of memory
on the GPU is an expennse synchronization point,  the
memory space for internal work areas must be preallocated for all
invocations  of the preconditioner; allocation of auxiliary data areas at
each invocation of the prevonditioner is perfectly doable on the CPU
side, but not  on the GPU. 

The second issue has to do with the implementation of the
matrix-vector product on the GPU: as mentioned
in~\cite{Filippone:2017:SMM:3034774.3017994}, to run at full speed on
the GPU, the matrices must be  sufficiently large to keep all
computing units busy, and this is very difficult at the coarse level
of the preconditioner hierarchy because the size of the aggregates is
small. 

The third issue is related to the efficiency of communication between
the various computing nodes; to proceed with the computation of the
matrix-vector product in parallel between the different nodes, it is
necessary to exchange the data of the \emph{halo}. For good data
partitions, the amount of data to be exchanged displays a
surface-to-volume effect, and therefore large products can be computed
effectively; however when moving between different levels of the
preconditioning hierarchy we are precisely going towards smaller
matrices, hence the surface-to-volume ratio and the efficiency of the
parallelization decreases. % The speed ratio of the GPU with respect to
% the CPU only makes things worse; again, it may be necessary to limit
% the number of levels to less than what would be efficient when using
% CPUs. 